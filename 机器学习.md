# 一、基础

## 1.1 定义

​	从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。

## 1.2 一般过程

![image-20200220193108039](C:%5CUsers%5C11979%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200220193108039.png)

## 1.3 数据预处理

​	将未加工数据转化成适合分析的形式，包括：多数据源的数据融合，数据清洗，维度规约。

功能：

1. 数据集成
2. 数据变换
3. 数据清洗

## 1.4 数据降维

​	将数据从高维特征空间向低维特征空间映射的过程。

​	维度降低也就意味着可视化变得更容易与清晰了，同时将有效信息提取，摈弃无用信息。

​	好处概括：方便数据可视化+数据分析+数据压缩+数据提取。

### 方式

![image-20200220193854570](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200220193854570.png)

## 1.5 常见算法

![image-20200220193931443](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200220193931443.png)

## 1.6 常见问题

1. 数据量过多或过少，数据不均衡
2. 维度灾难
3. 数据不完整
4. 异常，重复数据
5. 数据不一致

## 1.7 常见陷阱

1. 错误理解相关关系，错把相关当因果关系
2. 错误的比较对象
3. 数据抽样
4. 忽略或过于关注极值
5. 相信巧合数据
6. 数据未做归一化
7. 忽视第三方数据
8. 过度关心统计指标

# 二、分类算法

## 2.1 决策树

​	利用训练样本集获得分类函数。

​	决策树由决策节点，分支，叶子节点组成。

1. 决策节点表示在样本的一个属性上进行的划分。
2. 分支表示对于决策节点进行划分的输出。
3. 叶节点代表分支到达的类。

## 2.2 ID3算法

​	使用**启发式算法**进行决策树的构造，贪心选取最有区分性的决策作为决策节点。

​	具体在每个节点处选择能获得最高信息增益的分支属性进行分裂，为了将整个决策树的样本纯度提高。

衡量样本集合纯度的指标为熵：

​	$Entropy(S)=-\sum _{i=1}^mp_i*log_2(p_i),p_i=|C_i|/n$。用频率估计概率

计算分支属性对于样本集分类好坏程度的度量--信息增益：

![image-20200228123514235](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228123514235.png)

## 2.3 C4.5算法

引入**信息增益率**作为度量（ID3若有两个度量是相差不大的，但由于样本数量的不同导致结果的不同，故C4.5采用**率**作为度量，为了减少个别样本数过少的情况）：

​	![image-20200228123931406](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228123931406.png)

## 2.4 CART算法

​	采用二分循环分割的方法，使生成的决策树的每个决策节点均有两个分支，构造出一颗二叉树。

度量指标为Gini指标：

​	![image-20200228124300015](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228124300015.png)

## 连续属性离散化，过拟合问题

​	![image-20200228124721779](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228124721779.png)

过拟合现象会导致随着决策树的继续增长，尽管训练误差仍在下降，但泛化误差停止下降，甚至提高

解决方法：

1. 错误率降低剪枝（REP）
2. 分类效果评价，准确率，精确率，召回率
3. 保留法
4. 蒙特卡洛交叉验证
5. k折交叉验证

## 2.5 集成学习

​	将多种学习方法组合获取比原方法更优的结果。

1. 装袋法
2. 提升法，给每个样本一个权重，进行T论训练
3. 随机森林

# 三、 聚类算法

## 3.1 聚类分析

![image-20200313144253602](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144253602.png)

![image-20200313144258090](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144258090.png)

![image-20200313144302396](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144302396.png)

![image-20200313144306266](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144306266.png)

## 3.2 外部指标

![image-20200313144327870](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144327870.png)

![image-20200313144330750](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144330750.png)

![image-20200313144335033](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144335033.png)

## 3.3 内部指标

![image-20200313144344199](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144344199.png)

![image-20200313144347223](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144347223.png)

![image-20200313144350500](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144350500.png)

![image-20200313144353701](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144353701.png)

![image-20200313144357010](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144357010.png)

![image-20200313144402143](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144402143.png)

![image-20200313144449179](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144449179.png)

![image-20200313144452717](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200313144452717.png)

## 3.4 基于分类的方法

1. 基于划分的方法是简单、常用的一种聚类方法。

    通过将对象划分为互斥的簇进行聚类， 每个对象属于且仅属于一个簇。

    划分结果旨在使簇之间的相似性低，簇内部的相似度高。

    基于划分的方法常用算法有k均值、k‐medoids、k‐prototype等。后俩是前面的改进。

2. k-均值算法

    (1)	def:k‐均值聚类是:基于划分的聚类算法，计算样本点与类簇质心的距离，与类簇质心相近的样本点划分为同一类簇。k‐均值通过样本间的距离来衡量它们之间的相似度，两个样本距离越远，则相似度越低，否则相似度越高。

    (2)	步骤：k‐均值算法聚类步骤如下： 

    ​	①首先选取k个类簇（k需要用户进行指定）的质心，通常是随机选取。

    ​	②对剩余的每个样本点，计算它们到各个质心的欧式距离，并将其归入到相互间距离最小的质心所在的簇。计算各个新簇的质心。

    ​	③在所有样本点都划分完毕后，根据划分情况重新计算各个簇的质心所在位置，然后迭代计算各个样本点到各簇质心的距离，对所有样本点重新进行划分。

    ​	④重复第②步和第③步, 直到迭代计算后，所有样本点的划分情况保持不变，此时说明k‐均值算法已经得到了最优解，将运行结果返回。

    (3)	 优缺点：

    ​	①k‐均值算法原理简单，容易实现，且运行效率比较高 

    ​	②k‐均值算法聚类结果容易解释，适用于高维数据的聚类 

    ​	③k‐均值算法采用贪心策略，导致容易局部收敛，在大规模数据集上求解较慢 

    ​	①k‐均值算法对离群点和噪声点非常敏感，少量的离群点和噪声点可能对算法求平均值产生极大影响，从而	影响聚类结果

    ​	②k‐均值算法中初始聚类中心的选取也对算法结果影响很大，不同的初始中心可能会导致不同的聚类结果。	对此，研究人员提出k‐均值++算法，其思想是使初始的聚类中心之间的相互距离尽可能远。

    (4) 需注意：

    ​	- 模型的输入数据为数值型数据（如果是离散变量，需要作哑变量处理

    ​	– 需要将原始数据作标准化处理（防止不同量纲对聚类产生影响）

    (5) k值的选取：

    ​	– 与层次聚类算法结合，先通过层次聚类算法得出大致的聚类数目，并且获得一个初始聚类结果，然后再通过k‐均值算法改进聚类结果.

    ​	– 基于系统演化的方法，将数据集视为伪热力学系统，在分裂和合并过程中，将系统演化到稳定平衡状态从而确定k值.

3. k-均值++算法

    由于k‐均值算法不适用于非凸面形状（非球形）的数据集。

    k-均值++步骤：

    ![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image002.jpg)

4. k-medoids算法

    ​	由于k-均值算法太受噪声的影响，k ‐medoids算法不通过计算簇中所有样本的平均值得到簇的中心，而是通过选取原有样本中的样本点作为代表对象代表这个簇，计算剩下的样本点与代表对象的距离，将样本点划分到与其距离最近的代表对象所在的簇中。

    ​	距离计算过程与k均值算法的计算过程类似，只是将距离度量中的中心替换为代表对象，绝对误差标准如下：

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image004.jpg)

​		围绕中心点划分(Partitioning Around Mediods, PAM)算法是k‐medoids聚类的一种典型实现。

5.k-prototype算法

​	k‐prototype算法的聚类过程与k‐均值算法相同，只是在聚类过程中引入参数r来控制数值属性和分类属性的权重。

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image006.jpg)

## 3.5 基于密度聚类的方法

1. 基于划分聚类和基于层次聚类的方法在聚类过程中根据距离来划分类簇，因此只能够用于挖掘球状簇。

    ​	为了解决这一缺陷，基于密度聚类算法利用密度思想，将样本中的高密度区域(即样本点分布稠密的区域)划分为簇，将簇看作是样本空间中被稀疏区域(噪声)分隔开的稠密区域。这一算法的主要目的是过滤样本空间中的稀疏区域，获取稠密区域作为簇。

    ​	基于密度的聚类算法是根据密度而不是距离来计算样本相似度，所以基于密度的聚类算法能够用于挖掘任意形状的簇，并且能够有效过滤掉噪声样本对于聚类结果的影响。

    ​	常见的基于密度的聚类算法有DBSCAN、OPTICS和DENCLUE等。其中，OPTICS 对DBSCAN算法进行了改进，降低了对输入参数的敏感程度。DENCLUE算法综合了基于划分、基于层次的方法。

2.  DBSCAN算法

    ​	DBSCAN采用基于中心的密度定义，样本的密度通过核心对象在߳半径内的样本点个数（包括自身）来估计。

    ​	基于领域来描述样本的密度。

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image002-1584083385717.jpg)

​			相关概念：

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image004-1584083385718.jpg)

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image006-1584083385718.jpg)

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image008.jpg)

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image010.jpg)

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image012.jpg)

​		DBSCAN算法根据密度可达关系求出所有密度相连样本的最大集合，将这些样本点作为同一个簇。DBSCAN算	法任意选取一个核心对象作为“种子”，然后从“种子”出发寻找所有密度可达的其他核心对象，并且包含每个核心	对象的*ε*‐邻域的非核心对象，将这些核心对象和非核心对象作为一个簇。当寻找完成一个簇之后，选择还没有簇	标记的其他核心对象，得到一个新的簇，反复执行这个过程，直到所有的核心对象都属于某一个簇为止。

​		优缺点：

​			①DBSCAN可以用于对任意形状的稠密数据集进行聚类，DBSCAN算法对输入顺序不敏感。DBSCAN能够在	聚类的过程中发现数据集中的噪声点，且算法本身对噪声不敏感。当数据集分布为非球型时，使用DBSCAN算法	效果较好。

​			②DBSCAN算法要对数据集中的每个对象进行邻域检查，当数据集较大时，聚类收敛时间长，需要较大的	内存支持，I/O消耗也很大，此时可以采用KD树或球树对算法进行改进，快速搜索最近邻，帮助算法快速收敛。	此外，当空间聚类的密度不均匀，聚类间距离相差很大时，聚类的质量较差。

​			③DBSCAN算法的聚类结果受到邻域参数![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image014.jpg)的影响较大不同的输入

​	参数对聚类结果有很大的影响，邻域参数也需要人工输入，调参时需要对

​	两个参数联合调参，比较复杂。

## 3.6 基于层次聚类的方法

1.  层次聚类的应用广泛程度仅次于基于划分的聚类，核心思想就是通过对数 据集按照层次，把数据划分到不同层的簇，从而形成一个树形的聚类结构。 

    ​	层次聚类算法可以揭示数据的分层结构，在树形结构上不同层次进行划分，可以得到不同粒度的聚类结果。按照层次聚类的过程分为自底向上的聚合聚类和自顶向下的分裂聚类。聚合聚类以AGNES、BIRCH、ROCK等算法为代表，分裂聚类以DIANA算法为代表。

    ​	自底向上的聚合聚类将每个样本看作一个簇，初始状态下簇的数目等于样 本的数目，然后根据算法的规则对样本进行合并，直到满足算法的终止条件。自顶向下的分裂聚类先将所有样本看作属于同一个簇，然后逐渐分裂成更小的簇，直到满足算法终止条件为止。目前大多数是自底向上的聚合聚类，自顶向下的分裂聚类比较少。

![img](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/clip_image002-1584083473198.jpg)

 