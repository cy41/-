# 一、基础

## 1.1 定义

​	从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。

## 1.2 一般过程

![image-20200220193108039](C:%5CUsers%5C11979%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200220193108039.png)

## 1.3 数据预处理

​	将未加工数据转化成适合分析的形式，包括：多数据源的数据融合，数据清洗，维度规约。

功能：

1. 数据集成
2. 数据变换
3. 数据清洗

## 1.4 数据降维

​	将数据从高维特征空间向低维特征空间映射的过程。

​	维度降低也就意味着可视化变得更容易与清晰了，同时将有效信息提取，摈弃无用信息。

​	好处概括：方便数据可视化+数据分析+数据压缩+数据提取。

### 方式

![image-20200220193854570](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200220193854570.png)

## 1.5 常见算法

![image-20200220193931443](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200220193931443.png)

## 1.6 常见问题

1. 数据量过多或过少，数据不均衡
2. 维度灾难
3. 数据不完整
4. 异常，重复数据
5. 数据不一致

## 1.7 常见陷阱

1. 错误理解相关关系，错把相关当因果关系
2. 错误的比较对象
3. 数据抽样
4. 忽略或过于关注极值
5. 相信巧合数据
6. 数据未做归一化
7. 忽视第三方数据
8. 过度关心统计指标

# 二、分类算法

## 2.1 决策树

​	利用训练样本集获得分类函数。

​	决策树由决策节点，分支，叶子节点组成。

1. 决策节点表示在样本的一个属性上进行的划分。
2. 分支表示对于决策节点进行划分的输出。
3. 叶节点代表分支到达的类。

## 2.2 ID3算法

​	使用**启发式算法**进行决策树的构造，贪心选取最有区分性的决策作为决策节点。

​	具体在每个节点处选择能获得最高信息增益的分支属性进行分裂，为了将整个决策树的样本纯度提高。

衡量样本集合纯度的指标为熵：

​	$Entropy(S)=-\sum _{i=1}^mp_i*log_2(p_i),p_i=|C_i|/n$。用频率估计概率

计算分支属性对于样本集分类好坏程度的度量--信息增益：

![image-20200228123514235](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228123514235.png)

## 2.3 C4.5算法

引入**信息增益率**作为度量（ID3若有两个度量是相差不大的，但由于样本数量的不同导致结果的不同，故C4.5采用**率**作为度量，为了减少个别样本数过少的情况）：

​	![image-20200228123931406](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228123931406.png)

## 2.4 CART算法

​	采用二分循环分割的方法，使生成的决策树的每个决策节点均有两个分支，构造出一颗二叉树。

度量指标为Gini指标：

​	![image-20200228124300015](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228124300015.png)

## 连续属性离散化，过拟合问题

​	![image-20200228124721779](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20200228124721779.png)

过拟合现象会导致随着决策树的继续增长，尽管训练误差仍在下降，但泛化误差停止下降，甚至提高

解决方法：

1. 错误率降低剪枝（REP）
2. 分类效果评价，准确率，精确率，召回率
3. 保留法
4. 蒙特卡洛交叉验证
5. k折交叉验证

## 2.5 集成学习

​	将多种学习方法组合获取比原方法更优的结果。

1. 装袋法
2. 提升法，给每个样本一个权重，进行T论训练
3. 随机森林